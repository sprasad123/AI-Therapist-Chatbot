# -*- coding: utf-8 -*-
"""AI Therapist Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xe0SWiL-Dav0TMawEhFE_kcN1ldxcYX4
"""

!export CUDA_LAUNCH_BLOCKING=1

!pip install transformers
!pip install datasets fsspec gcsfs

from transformers import pipeline

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
from transformers import DataCollatorForSeq2Seq

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

from transformers import EarlyStoppingCallback

import pandas as pd

from datasets import Dataset
import torch

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/My Drive/CMPE258DL/dataset/'
results_data_dir = '/content/drive/My Drive/CMPE258DL/'

csv_path = data_dir + "balanced_training_dataset.csv"
training_data = pd.read_csv(csv_path)

csv_path = data_dir + "validation_dataset.csv"
validation_data = pd.read_csv(csv_path)

csv_path = data_dir + "test_dataset.csv"
test_data = pd.read_csv(csv_path)

model_save_path = "/content/drive/My Drive/CMPE258DL/fine_tuned_blenderbot_5_epochs_final"

print(test_data['instruction'].isna().sum())  # Count NaN values
print((test_data['instruction'] == "").sum())  # Count empty strings

print(training_data.dtypes)
print(training_data[['instruction', 'output']].head())

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the model and tokenizer
# fine_tuned_model = BlenderbotForConditionalGeneration.from_pretrained(model_save_path)
# fine_tuned_tokenizer = BlenderbotTokenizer.from_pretrained(model_save_path)

# print("Fine-tuned model and tokenizer loaded successfully!")

mname = "facebook/blenderbot-400M-distill"
tokenizer = BlenderbotTokenizer.from_pretrained(mname)
model = BlenderbotForConditionalGeneration.from_pretrained(mname)

# Ensure consistent max_length
tokenizer.model_max_length = 512
model.config.max_length = 256  # Maximum length for generation

# Check if the tokenizer already has a pad token
if tokenizer.pad_token is None:
   print("Adding padding token '<pad>' to the tokenizer.")
   tokenizer.add_special_tokens({'pad_token': '<pad>'})  # Add <pad> token if missing

# Resize model embeddings to match the tokenizer's vocabulary size
model.resize_token_embeddings(len(tokenizer))
print(f"Resized model embeddings to match tokenizer vocab size: {len(tokenizer)}")

def tokenize(row):
    # Convert instruction and output to strings
    instruction = str(row['instruction']) if pd.notna(row['instruction']) else ""
    output = str(row['output']) if pd.notna(row['output']) else ""

    # Tokenize the instruction and output
    inputs = tokenizer(instruction, truncation=True, padding="max_length", max_length=128, return_tensors="pt")
    targets = tokenizer(output, truncation=True, padding="max_length", max_length=128, return_tensors="pt")

    labels = targets["input_ids"].squeeze(0)
    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding in loss calculation

    return {"input_ids": inputs["input_ids"].squeeze(0).tolist(), "attention_mask": \
            inputs["attention_mask"].squeeze(0).tolist(), "labels": labels.tolist()}

# Tokenizing the datasets
tokenized_training_data = [tokenize(row) for row in training_data.to_dict("records")]
tokenized_validation_data = [tokenize(row) for row in validation_data.to_dict("records")]
tokenized_test_data = [tokenize(row) for row in test_data.to_dict("records")]

# Convert tokenized data to Dataset objects
train_dataset = Dataset.from_pandas(pd.DataFrame(tokenized_training_data))
eval_dataset = Dataset.from_pandas(pd.DataFrame(tokenized_validation_data))
test_dataset = Dataset.from_pandas(pd.DataFrame(tokenized_test_data))

print(train_dataset)
print(eval_dataset)
print(test_dataset)

#Trainer arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",          # Output directory for model checkpoints
    eval_strategy="epoch",           # Evaluate after each epoch
    save_strategy="epoch",           # Save model at the end of each epoch
    learning_rate=5e-5,              # Learning rate
    per_device_train_batch_size=1,   # Batch size for training
    per_device_eval_batch_size=1,    # Batch size for evaluation
    weight_decay=0.01,               # Weight decay
    save_total_limit=3,              # Total number of checkpoints saved
    num_train_epochs=1,              # Number of training epochs
    predict_with_generate=True,      # Use generate during evaluation
    fp16=True,                       # Use mixed precision
    logging_dir='./logs',            # Log
    load_best_model_at_end=True      # Load the best model at the end of training
)

# Trainer
trainer = Seq2SeqTrainer(model=model, args=training_args, train_dataset=train_dataset, \
                         eval_dataset=test_dataset, tokenizer=tokenizer, \
                         #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping with patience=3
)

# Train the model
trainer.train()
print("After training")
#!nvidia-smi

# Save fine-tuned model and tokenizer
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

print(f"Model and tokenizer saved to {model_save_path}")

# Get log history
log_history = trainer.state.log_history

# Separate training and validation loss
training_loss = [entry["loss"] for entry in log_history if "loss" in entry]
validation_loss = [entry["eval_loss"] for entry in log_history if "eval_loss" in entry]
steps = [entry["step"] for entry in log_history if "loss" in entry]

# Plot the losses
plt.figure(figsize=(10, 6))
plt.plot(range(len(training_loss)), training_loss, label="Training Loss")
plt.plot(range(len(validation_loss)), validation_loss, label="Validation Loss", linestyle="--")
plt.xlabel("Steps (or Epochs for Validation)")
plt.ylabel("Loss")
plt.title("Training and Validation Loss Curves (Blenderbot 5 epochs)")
plt.legend()
plt.grid()
plt.savefig('Train_val_loss_curve.png')
plt.show()

# Create a copy of the test data
test_data_with_responses = test_data.copy()

def generate_response(instruction):
    # Truncate input if it exceeds the model's maximum length
    if len(instruction) >= tokenizer.model_max_length:
        instruction = instruction[:tokenizer.model_max_length - 2]

    inputs = tokenizer(
        instruction,
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    # Move input tensors to the same device as the model
    inputs = {key: value.to(device) for key, value in inputs.items()}
    # Generate the response with adjusted parameters
    reply_ids = model.generate(
        **inputs,
        temperature=0.4,
        max_length=256,
        num_beams=5,
        early_stopping=True,
        eos_token_id=tokenizer.eos_token_id,
    )
    response = tokenizer.decode(reply_ids[0], skip_special_tokens=True)
    return response

# Apply response generation to test data
def generate_response_with_logging(instruction):
    try:
        if not isinstance(instruction, str) or len(instruction.strip()) == 0:
            raise ValueError("Invalid input: Input must be a non-empty string.")
        # Check input length before processing
        if len(instruction) > tokenizer.model_max_length - 2:
            print(f"Input too long, truncating: {instruction[:50]}...")
            instruction = instruction[:tokenizer.model_max_length - 2]
        print(f"Processing input of length {len(instruction)}")
        inputs = tokenizer(instruction, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        print(f"Tokenized inputs: {inputs}")
        return generate_response(instruction)
    except Exception as e:
        print(f"Error with input: {instruction[:50]}... -> {e}")
        return "Error"

test_data_with_responses['model_response'] = test_data_with_responses['instruction'].apply(generate_response_with_logging)

# Save the updated test dataset
output_path = results_data_dir + "test_data_with_responses.csv"
test_data_with_responses.to_csv(output_path, index=False)
print("Responses saved to 'test_data_with_responses.csv'")